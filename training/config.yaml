# TODO:
# Define training hyperparameters:
# - vocab_size
# - n_layer, n_head, n_embd
# - learning_rate, batch_size, block_size, max_iters
# - device (cpu or cuda)

model:
  vocab_size: 16000
  n_layer: 6
  n_head: 6
  n_embd: 512
  block_size: 256
  dropout: 0.1
train:
  learning_rate: 0.0003
  batch_size: 16
  max_iters: 100000
  eval_interval: 1000
  eval_iters: 100
  log_interval: 100
  device: cpu
